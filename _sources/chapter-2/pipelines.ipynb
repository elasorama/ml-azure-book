{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines \n",
    "\n",
    "In azure ML, a pipeline is a collection of components that depend on each other's outputs and run sequentially.\n",
    "\n",
    "A good when trying to combine the components together is to always start with a `test-first` approach and create an end to end pipeline on the local machine. Only after the tests have finished running, we can try and run the pipeline on the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a local pipeline \n",
    "\n",
    "Let us define three simple components:\n",
    "\n",
    "1. The first one accepts a string and cleans it by removing all the punctuation marks and converting it to lower case.\n",
    "2. The second one accepts a string and returns a list of words.\n",
    "3. The third one accepts a list of words and returns a dictionary with the word count.\n",
    "\n",
    "Let us start by creating a folder called `components` and add the following files and folders:\n",
    "\n",
    "```bash\n",
    "components\n",
    "├── clean\n",
    "│   ├── __init__.py\n",
    "│   ├── component.py\n",
    "│   └── run.yaml\n",
    "├── count\n",
    "│   ├── __init__.py\n",
    "│   ├── component.py\n",
    "│   └── run.yaml\n",
    "└── tokenize\n",
    "    ├── __init__.py\n",
    "    ├── component.py\n",
    "    └── run.yaml\n",
    "```\n",
    "\n",
    "**clean component:**\n",
    "\n",
    "```python\n",
    "# Importing regex\n",
    "import re\n",
    "\n",
    "# Argument parsing \n",
    "import argparse\n",
    "\n",
    "def main(txt_file: str, txt_file_out: str): \n",
    "    \"\"\"\n",
    "    Function to clean the incoming string \n",
    "    \n",
    "    The cleaning removes punctuations, lowercases the string and strips it \n",
    "    \"\"\"\n",
    "    # Define the list to store the strings\n",
    "    list_of_strings = []\n",
    "\n",
    "    # Open the file (replace 'your_file.txt' with your file name)\n",
    "    with open(txt_file, 'r') as file:\n",
    "        # Read each line in the file\n",
    "        for line in file:\n",
    "            # Strip the newline character and append to the list\n",
    "            list_of_strings.append(line)\n",
    "\n",
    "    # Clean the strings\n",
    "    for x in list_of_strings:\n",
    "        # Remove punctuations\n",
    "        x = re.sub(r'[^\\w\\s]', '', x)\n",
    "        # Lowercase the string\n",
    "        x = x.lower()\n",
    "        # Strip the string\n",
    "        x = x.strip()\n",
    "\n",
    "    # Writing the cleaned strings to a file\n",
    "    with open(txt_file_out, 'w') as file:\n",
    "        for x in list_of_strings:\n",
    "            file.write(x)\n",
    "\n",
    "    return \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Parsing the arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--txt_file\", help=\"Input txt file\")\n",
    "    parser.add_argument(\"--txt_file_out\", help=\"Output txt file\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Calling the main function\n",
    "    main(txt_file=args.txt_file)\n",
    "```\n",
    "\n",
    "**tokenization component:**\n",
    "\n",
    "```python\n",
    "# Argument parsing \n",
    "import argparse\n",
    "\n",
    "def main(txt_file: str, txt_file_out: str): \n",
    "    \"\"\"\n",
    "    Function to tokenize the incoming string \n",
    "    \n",
    "    The tokenization splits the string into words\n",
    "    \"\"\"\n",
    "    # Define the list to store the strings\n",
    "    list_of_strings = []\n",
    "\n",
    "    # Open the file (replace 'your_file.txt' with your file name)\n",
    "    with open(txt_file, 'r') as file:\n",
    "        # Read each line in the file\n",
    "        for line in file:\n",
    "            # Strip the newline character and append to the list\n",
    "            list_of_strings.append(line)\n",
    "\n",
    "    # Tokenize the strings\n",
    "    for x in list_of_strings:\n",
    "        # Split the string into words\n",
    "        x = x.split()\n",
    "\n",
    "    # Writing the tokenized strings to a file\n",
    "    with open(txt_file_out, 'w') as file:\n",
    "        for x in list_of_strings:\n",
    "            file.write(x)\n",
    "\n",
    "    return\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Parsing the arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--txt_file\", help=\"Input txt file\")\n",
    "    parser.add_argument(\"--txt_file_out\", help=\"Output txt file\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Calling the main function\n",
    "    main(txt_file=args.txt_file, txt_file_out=args.txt_file_out)\n",
    "```\n",
    "\n",
    "**count component:**\n",
    "\n",
    "```python\n",
    "# Argument parsing\n",
    "import argparse\n",
    "\n",
    "# Counting function \n",
    "from collections import Counter\n",
    "\n",
    "def main(txt_file: str, txt_file_out: str):\n",
    "    \"\"\"\n",
    "    Function to count the incoming tokens\n",
    "    \"\"\"\n",
    "    # Define the list to store the strings\n",
    "    list_of_strings = []\n",
    "\n",
    "    # Open the file (replace 'your_file.txt' with your file name)\n",
    "    with open(txt_file, 'r') as file:\n",
    "        # Read each line in the file\n",
    "        for line in file:\n",
    "            # Strip the newline character and append to the list\n",
    "            list_of_strings.append(line)\n",
    "\n",
    "    # Creating a counter \n",
    "    counter = Counter(list_of_strings)\n",
    "\n",
    "    # Sorting by most common\n",
    "    token_counts = counter.most_common()\n",
    "\n",
    "    # Writing the token counts to a file\n",
    "    with open(txt_file_out, 'w') as file:\n",
    "        for x in token_counts:\n",
    "            file.write(f\"{x[0]}: {x[1]}\\n\")\n",
    "\n",
    "    return\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Parsing the arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--txt_file\", help=\"Input txt file\")\n",
    "    parser.add_argument(\"--txt_file_out\", help=\"Output txt file\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Calling the main function\n",
    "    main(txt_file=args.txt_file, txt_file_out=args.txt_file_out)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let us define a list of strings and save them into a txt file \n",
    "# called \"list_of_strings.txt\"\n",
    "list_of_strings = [\"Hypercube is awesome\", \"Flexitricity is amazing too\"]\n",
    "\n",
    "# Now let us save the list of strings into a txt file\n",
    "with open(\"list_of_strings.txt\", \"w\") as f:\n",
    "    for string in list_of_strings:\n",
    "        f.write(string + \"\\n\")\n",
    "\n",
    "# Now let us import the components \n",
    "from components.clean.component import main as clean\n",
    "from components.tokenize.component import main as tokenize\n",
    "from components.count.component import main as count\n",
    "\n",
    "# Wrapping the components into a pipeline\n",
    "def main(\n",
    "        input_file: str, \n",
    "        output_file_cleaned: str,\n",
    "        output_file_tokenized: str,\n",
    "        output_file_counted: str\n",
    "):\n",
    "    clean(txt_file=input_file, txt_file_out=output_file_cleaned)\n",
    "    tokenize(txt_file=output_file_cleaned, txt_file_out=output_file_tokenized)\n",
    "    count(txt_file=output_file_tokenized, txt_file_out=output_file_counted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us test the pipeline localy using the triple A pattern: \n",
    "\n",
    "1. **Arrange**: Create the inputs for the pipeline\n",
    "2. **Act**: Run the pipeline\n",
    "3. **Assert**: Check the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypercube is awesome\n",
      "Flexitricity is amazing too\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets print the input file \n",
    "with open(\"list_of_strings.txt\", \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arranging \n",
    "input_file = 'list_of_strings.txt'\n",
    "output_file_cleaned = 'list_of_strings_cleaned.txt'\n",
    "output_file_tokenized = 'list_of_strings_tokenized.txt'\n",
    "output_file_counted = 'list_of_strings_counted.txt'\n",
    "\n",
    "# Acting\n",
    "main(\n",
    "    input_file=input_file, \n",
    "    output_file_cleaned=output_file_cleaned,\n",
    "    output_file_tokenized=output_file_tokenized,\n",
    "    output_file_counted=output_file_counted\n",
    ")\n",
    "\n",
    "# Asserting that the first row is \"is: 2\"\n",
    "with open(output_file_counted, \"r\") as f:\n",
    "    assert f.readline().strip() == \"is: 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is: 2\n",
      "hypercube: 1\n",
      "awesome: 1\n",
      "flexitricity: 1\n",
      "amazing: 1\n",
      "too: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing the output file\n",
    "with open(output_file_counted, \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the pipeline working localy, we can start creating the pipeline on the cloud. Keep in mind, the the only things that will change will be the local file paths will change to the file paths in azure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline on the cloud"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
