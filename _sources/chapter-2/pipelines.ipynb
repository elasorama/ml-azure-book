{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines \n",
    "\n",
    "In azure ML, a pipeline is a collection of components that depend on each other's outputs and run sequentially.\n",
    "\n",
    "A good when trying to combine the components together is to always start with a `test-first` approach and create an end to end pipeline on the local machine. Only after the tests have finished running, we can try and run the pipeline on the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a local pipeline \n",
    "\n",
    "Let us define three simple components:\n",
    "\n",
    "1. The first one accepts a string and cleans it by removing all the punctuation marks and converting it to lower case.\n",
    "2. The second one accepts a string and returns a list of words.\n",
    "3. The third one accepts a list of words and returns a dictionary with the word count.\n",
    "\n",
    "Let us start by creating a folder called `components` and add the following files and folders:\n",
    "\n",
    "```bash\n",
    "components\n",
    "├── clean\n",
    "│   ├── __init__.py\n",
    "│   ├── component.py\n",
    "│   └── run.yaml\n",
    "├── count\n",
    "│   ├── __init__.py\n",
    "│   ├── component.py\n",
    "│   └── run.yaml\n",
    "└── tokenize\n",
    "    ├── __init__.py\n",
    "    ├── component.py\n",
    "    └── run.yaml\n",
    "```\n",
    "\n",
    "**clean component:**\n",
    "\n",
    "```python\n",
    "# Importing regex\n",
    "import re\n",
    "\n",
    "# Argument parsing \n",
    "import argparse\n",
    "\n",
    "def main(txt_file: str, txt_file_out: str): \n",
    "    \"\"\"\n",
    "    Function to clean the incoming string \n",
    "    \n",
    "    The cleaning removes punctuations, lowercases the string and strips it \n",
    "    \"\"\"\n",
    "    # Define the list to store the strings\n",
    "    list_of_strings = []\n",
    "\n",
    "    # Open the file (replace 'your_file.txt' with your file name)\n",
    "    with open(txt_file, 'r') as file:\n",
    "        # Read each line in the file\n",
    "        for line in file:\n",
    "            # Strip the newline character and append to the list\n",
    "            list_of_strings.append(line)\n",
    "\n",
    "    # Clean the strings\n",
    "    for x in list_of_strings:\n",
    "        # Remove punctuations\n",
    "        x = re.sub(r'[^\\w\\s]', '', x)\n",
    "        # Lowercase the string\n",
    "        x = x.lower()\n",
    "        # Strip the string\n",
    "        x = x.strip()\n",
    "\n",
    "    # Writing the cleaned strings to a file\n",
    "    with open(txt_file_out, 'w') as file:\n",
    "        for x in list_of_strings:\n",
    "            file.write(x)\n",
    "\n",
    "    return \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Parsing the arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--txt_file\", help=\"Input txt file\")\n",
    "    parser.add_argument(\"--txt_file_out\", help=\"Output txt file\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Calling the main function\n",
    "    main(txt_file=args.txt_file)\n",
    "```\n",
    "\n",
    "**tokenization component:**\n",
    "\n",
    "```python\n",
    "# Argument parsing \n",
    "import argparse\n",
    "\n",
    "def main(txt_file: str, txt_file_out: str): \n",
    "    \"\"\"\n",
    "    Function to tokenize the incoming string \n",
    "    \n",
    "    The tokenization splits the string into words\n",
    "    \"\"\"\n",
    "    # Define the list to store the strings\n",
    "    list_of_strings = []\n",
    "\n",
    "    # Open the file (replace 'your_file.txt' with your file name)\n",
    "    with open(txt_file, 'r') as file:\n",
    "        # Read each line in the file\n",
    "        for line in file:\n",
    "            # Strip the newline character and append to the list\n",
    "            list_of_strings.append(line)\n",
    "\n",
    "    # Tokenize the strings\n",
    "    for x in list_of_strings:\n",
    "        # Split the string into words\n",
    "        x = x.split()\n",
    "\n",
    "    # Writing the tokenized strings to a file\n",
    "    with open(txt_file_out, 'w') as file:\n",
    "        for x in list_of_strings:\n",
    "            file.write(x)\n",
    "\n",
    "    return\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Parsing the arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--txt_file\", help=\"Input txt file\")\n",
    "    parser.add_argument(\"--txt_file_out\", help=\"Output txt file\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Calling the main function\n",
    "    main(txt_file=args.txt_file, txt_file_out=args.txt_file_out)\n",
    "```\n",
    "\n",
    "**count component:**\n",
    "\n",
    "```python\n",
    "# Argument parsing\n",
    "import argparse\n",
    "\n",
    "# Counting function \n",
    "from collections import Counter\n",
    "\n",
    "def main(txt_file: str, txt_file_out: str):\n",
    "    \"\"\"\n",
    "    Function to count the incoming tokens\n",
    "    \"\"\"\n",
    "    # Define the list to store the strings\n",
    "    list_of_strings = []\n",
    "\n",
    "    # Open the file (replace 'your_file.txt' with your file name)\n",
    "    with open(txt_file, 'r') as file:\n",
    "        # Read each line in the file\n",
    "        for line in file:\n",
    "            # Strip the newline character and append to the list\n",
    "            list_of_strings.append(line)\n",
    "\n",
    "    # Creating a counter \n",
    "    counter = Counter(list_of_strings)\n",
    "\n",
    "    # Sorting by most common\n",
    "    token_counts = counter.most_common()\n",
    "\n",
    "    # Writing the token counts to a file\n",
    "    with open(txt_file_out, 'w') as file:\n",
    "        for x in token_counts:\n",
    "            file.write(f\"{x[0]}: {x[1]}\\n\")\n",
    "\n",
    "    return\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Parsing the arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--txt_file\", help=\"Input txt file\")\n",
    "    parser.add_argument(\"--txt_file_out\", help=\"Output txt file\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Calling the main function\n",
    "    main(txt_file=args.txt_file, txt_file_out=args.txt_file_out)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let us define a list of strings and save them into a txt file \n",
    "# called \"list_of_strings.txt\"\n",
    "list_of_strings = [\"Hypercube is awesome\", \"Flexitricity is amazing too\"]\n",
    "\n",
    "# Now let us save the list of strings into a txt file\n",
    "with open(\"list_of_strings.txt\", \"w\") as f:\n",
    "    for string in list_of_strings:\n",
    "        f.write(string + \"\\n\")\n",
    "\n",
    "# Now let us import the components \n",
    "from components.clean.component import main as clean\n",
    "from components.tokenize.component import main as tokenize\n",
    "from components.count.component import main as count\n",
    "\n",
    "# Wrapping the components into a pipeline\n",
    "def main(\n",
    "        input_file: str, \n",
    "        output_file_cleaned: str,\n",
    "        output_file_tokenized: str,\n",
    "        output_file_counted: str\n",
    "):\n",
    "    clean(txt_file=input_file, txt_file_out=output_file_cleaned)\n",
    "    tokenize(txt_file=output_file_cleaned, txt_file_out=output_file_tokenized)\n",
    "    count(txt_file=output_file_tokenized, txt_file_out=output_file_counted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us test the pipeline localy using the triple A pattern: \n",
    "\n",
    "1. **Arrange**: Create the inputs for the pipeline\n",
    "2. **Act**: Run the pipeline\n",
    "3. **Assert**: Check the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypercube is awesome\n",
      "Flexitricity is amazing too\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets print the input file \n",
    "with open(\"list_of_strings.txt\", \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of strings: 2\n",
      "Number of cleaned strings: 2\n",
      "Output file: list_of_strings_cleaned.txt\n",
      "Reading file: list_of_strings_cleaned.txt\n",
      "Number of strings: 2\n",
      "Number of tokens: 7\n",
      "Output file: list_of_strings_tokenized.txt\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Arranging \n",
    "import pandas as pd \n",
    "input_file = 'list_of_strings.txt'\n",
    "output_file_cleaned = 'list_of_strings_cleaned.txt'\n",
    "output_file_tokenized = 'list_of_strings_tokenized.txt'\n",
    "output_file_counted = 'list_of_strings_counted.txt'\n",
    "\n",
    "# Acting\n",
    "main(\n",
    "    input_file=input_file, \n",
    "    output_file_cleaned=output_file_cleaned,\n",
    "    output_file_tokenized=output_file_tokenized,\n",
    "    output_file_counted=output_file_counted\n",
    ")\n",
    "\n",
    "# Asserting \n",
    "output_text = open(output_file_counted, \"r\").read().split(\"\\n\")\n",
    "assert output_text[0] == 'is: 2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the pipeline working localy, we can start creating the pipeline on the cloud. Keep in mind, the the only things that will change will be the local file paths will change to the file paths in azure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline on the cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the components working locally, we can start creating the pipeline on the cloud.\n",
    "\n",
    "The first step is to upload the component code and the metadata to the datastore. \n",
    "\n",
    "Then we can start connecting them in the azure ml `designer` tab. \n",
    "\n",
    "The typical metadata of our components is: \n",
    "\n",
    "```yaml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
    "type: command\n",
    "\n",
    "name: <component name>\n",
    "display_name: <component name in display>\n",
    "\n",
    "environment: azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu:1\n",
    "\n",
    "inputs:\n",
    "  txt_file:\n",
    "    type: uri_file  \n",
    "\n",
    "outputs:\n",
    "  txt_file_out:\n",
    "    type: uri_file\n",
    "\n",
    "command: >-\n",
    "  python component.py\n",
    "  --txt_file ${{inputs.txt_file}} \n",
    "  --txt_file_out ${{outputs.txt_file_out}}\n",
    "```\n",
    "\n",
    "The pipeline schema is: \n",
    "\n",
    "![pipeline schema](./images/simple_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the only thing we are missing is the very first input to the `Data Ingestor` component. Lets rectify that in the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a data asset for the pipeline \n",
    "\n",
    "The easiest way to create a data asset is to upload a .txt file to azure blob storage and from the ml studio register that file as a data asset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a a list of strings \n",
    "list_of_strings = [\"Hypercube is awesome\", \"Flexitricity is amazing too\", \"Machine Learning is the future\"]\n",
    "\n",
    "# Saving the list of strings into a txt file\n",
    "with open(\"list_of_strings_azure.txt\", \"w\") as f:\n",
    "    for string in list_of_strings:\n",
    "        f.write(string + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The uploading of the file can be done in several ways but the easiest is to use the azure storage explorer.\n",
    "\n",
    "![uploading a file](./images/data_asset.png)\n",
    "\n",
    "Now we can navigate to the Azure ML studio's `Data` tab and register the file as a data asset.\n",
    "\n",
    "![registering a data asset](./images/data_aset_created.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can return to the designer tab and add this data asset as an input to our pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full pipeline with the data asset input\n",
    "\n",
    "Having our data asset registered, we can now run the pipeline on the cloud. \n",
    "\n",
    "The full ran pipeline looks the following: \n",
    "\n",
    "![full pipeline](images/full_pipeline_green.png)\n",
    "\n",
    "The pipeline output is saved in azure via: \n",
    "\n",
    "https://electircityml3408517355.blob.core.windows.net/azureml-blobstore-9c2c557d-1e90-48ba-bbdf-b4c6c27e1ff1/azureml/a3d23810-9a96-43ea-83af-060400d4c806/txt_file_out \n",
    "\n",
    "The path is convoluted, but the contents of the file are the following: \n",
    "\n",
    "```txt\n",
    "is: 3\n",
    "hypercube: 1\n",
    "awesome: 1\n",
    "flexitricity: 1\n",
    "amazing: 1\n",
    "too: 1\n",
    "machine: 1\n",
    "learning: 1\n",
    "the: 1\n",
    "future: 1\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
