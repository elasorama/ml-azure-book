{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components \n",
    "\n",
    "## The basics of components \n",
    "\n",
    "A component in an Azure pipeline is a self-contained set of code that performs a specific task. Components are the building blocks of pipelines. \n",
    "\n",
    "Each component needs to be put in a separate directory and have an associated `run.yaml` file (the name can be different, but the extension should be `.yaml`).\n",
    "\n",
    "For example, if I have 3 components that: \n",
    "\n",
    "1. Splits the data into train and test sets\n",
    "2. Trains a model\n",
    "3. Evaluates the model\n",
    "\n",
    "Then the directory structure might be like in the `components/` directory: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "components/\n",
      "    eval/\n",
      "        component.py\n",
      "        run.yaml\n",
      "        __pycache__/\n",
      "            component.cpython-311.pyc\n",
      "    train/\n",
      "        component.py\n",
      "        run.yaml\n",
      "        __pycache__/\n",
      "            component.cpython-311.pyc\n",
      "    train_test_split/\n",
      "        component.py\n",
      "        run.yaml\n",
      "        __pycache__/\n",
      "            component.cpython-311.pyc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def list_files_and_subdirs(start_path):\n",
    "    for root, _, files in os.walk(start_path):\n",
    "        level = root.replace(start_path, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print(f'{indent}{Path(root).name}/')\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print(f'{subindent}{f}')\n",
    "\n",
    "# Example usage\n",
    "list_files_and_subdirs('components')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, each component has a .py and a .yaml file. The .py file contains the code for the component and the .yaml file contains the metadata for the component.\n",
    "\n",
    "For now, lets ignore the .yaml files and dive deeper into the `component.py` scripts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anatomy of a component\n",
    "\n",
    "The basic high level concept of a component is the following: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![component](./images/component.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A component can have a bunch of inputs, that are ingested into a python function and the function can create 1 or more outputs.\n",
    "\n",
    "The bellow code cell contains the file contents of the `components/train_test_split/component.py` file. \n",
    "\n",
    "```python\n",
    "\n",
    "# Sklearn data spliting \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Pandas for data manipulation\n",
    "import pandas as pd \n",
    "\n",
    "# Defining the component function \n",
    "def main(\n",
    "        input_file_path: str, \n",
    "        train_test_ratio: float, \n",
    "        train_output_path: str, \n",
    "        test_output_path: str\n",
    ") -> None: \n",
    "    \"\"\"\n",
    "    Splits the data into train and test sets.\n",
    "\n",
    "    Args: \n",
    "        input_file_path: Path to the data file\n",
    "        train_test_ratio: Percentage of the data to use for training\n",
    "        train_output_path: Path to the train data\n",
    "        test_output_path: Path to the test data\n",
    "    \"\"\"\n",
    "    # Infering the file type \n",
    "    file_type = input_file_path.split(\".\")[-1]\n",
    "\n",
    "    # Reading the input data\n",
    "    if file_type == \"csv\":\n",
    "        data = pd.read_csv(input_file_path)\n",
    "    elif file_type == \"json\":\n",
    "        data = pd.read_json(input_file_path)\n",
    "    elif file_type == \"xlsx\":\n",
    "        data = pd.read_excel(input_file_path)\n",
    "    elif file_type == 'parquet':\n",
    "        data = pd.read_parquet(input_file_path)\n",
    "    else:\n",
    "        raise Exception(\"Unsupported file type\")\n",
    "\n",
    "    data = pd.read_csv(input_file_path)\n",
    "    print(f\"Number of rows in all data: {data.shape[0]}\")\n",
    "\n",
    "    # Spliting the data\n",
    "    train, test = train_test_split(data, train_size=train_test_ratio)\n",
    "    print(f\"Number of rows in train data: {train.shape[0]}\")\n",
    "    print(f\"Number of rows in test data: {test.shape[0]}\")\n",
    "\n",
    "    # Saving the data (only to parquet for this example sake)\n",
    "    train.to_parquet(train_output_path, index=False)\n",
    "    test.to_parquet(test_output_path, index=False)\n",
    "\n",
    "    return\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    # Argument parsing \n",
    "    import argparse\n",
    "\n",
    "    # Parsing arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--input_file_path\", type=str, help=\"Path to the data file\")\n",
    "    parser.add_argument(\"--train_test_ratio\", type=float, help=\"Share of the data to use for training\")\n",
    "    parser.add_argument(\"--train_output_path\", type=str, help=\"Path to the train data\")\n",
    "    parser.add_argument(\"--test_output_path\", type=str, help=\"Path to the test data\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Calling the main function\n",
    "    main(\n",
    "        input_file_path=args.input_file_path, \n",
    "        train_test_ratio=args.train_test_ratio, \n",
    "        train_output_path=args.train_output_path, \n",
    "        test_output_path=args.test_output_path\n",
    "    )\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the above function is a simple function that takes in a path to a  dataframe and splits it into train and test sets. \n",
    "\n",
    "All the components need to be in scripts, thus the bottom part of a component script includes the argument parsing and the call to the main function.\n",
    "\n",
    "To run the component function, we need to call the component script with the arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in all data: 300\n",
      "Number of rows in train data: 240\n",
      "Number of rows in test data: 60\n"
     ]
    }
   ],
   "source": [
    "!python components/train_test_split/component.py \\\n",
    "    --input_file_path data/data.csv \\\n",
    "    --train_test_ratio 0.8 \\\n",
    "    --train_output_path data/train.parquet \\\n",
    "    --test_output_path data/test.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you have just run your first component! \n",
    "\n",
    "Now let us define the other components. \n",
    "\n",
    "`train/component.py`:\n",
    "\n",
    "```python \n",
    "# Linear regression model \n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Data reading \n",
    "import pandas as pd\n",
    "\n",
    "# Pickle file saving \n",
    "import pickle\n",
    "\n",
    "# Defining the component function\n",
    "def main(\n",
    "        input_data_path: str, \n",
    "        output_model_path: str\n",
    ") -> None: \n",
    "    \"\"\"\n",
    "    Trains a linear regression model.\n",
    "\n",
    "    Args: \n",
    "        input_data_path: Path to the data file\n",
    "        output_model_path: Path to the model\n",
    "    \"\"\"\n",
    "    # Reading the data\n",
    "    data = pd.read_parquet(input_data_path)\n",
    "    print(f\"Number of rows in all data: {data.shape[0]}\")\n",
    "\n",
    "    # Spliting the data\n",
    "    X = data.drop(\"y\", axis=1)\n",
    "    y = data[\"y\"]\n",
    "\n",
    "    # Training the model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Saving the model\n",
    "    with open(output_model_path, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    return\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Argument parsing \n",
    "    import argparse\n",
    "\n",
    "    # Parsing arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--input_data_path\", type=str, help=\"Path to the data file\")\n",
    "    parser.add_argument(\"--output_model_path\", type=str, help=\"Path to the model\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Calling the main function\n",
    "    main(\n",
    "        input_data_path=args.input_data_path,\n",
    "        output_model_path=args.output_model_path\n",
    "    )\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`eval/component.py`:\n",
    "\n",
    "```python\n",
    "# Pickle file loading \n",
    "import pickle \n",
    "\n",
    "# Data reading \n",
    "import pandas as pd\n",
    "\n",
    "# Defining the component function\n",
    "def main(\n",
    "        input_data_path: str, \n",
    "        input_model_path: str,\n",
    ") -> None: \n",
    "    \"\"\"\n",
    "    Evaluates a linear regression model.\n",
    "\n",
    "    Args: \n",
    "        input_data_path: Path to the data file\n",
    "        input_model_path: Path to the model\n",
    "        output_data_path: Path to the data with predictions\n",
    "    \"\"\"\n",
    "    # Reading the data\n",
    "    data = pd.read_parquet(input_data_path)\n",
    "    print(f\"Number of rows in all data: {data.shape[0]}\")\n",
    "\n",
    "    # Loading the model\n",
    "    with open(input_model_path, \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "\n",
    "    # Predicting the data\n",
    "    data[\"y_pred\"] = model.predict(data[['x']])\n",
    "\n",
    "    # Printing the MSE statistic \n",
    "    mse = ((data[\"y\"] - data[\"y_pred\"]) ** 2).mean()\n",
    "    print(f\"MSE: {mse}\")\n",
    "\n",
    "    return\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Argument parsing \n",
    "    import argparse\n",
    "\n",
    "    # Parsing arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--input_data_path\", type=str, help=\"Path to the data file\")\n",
    "    parser.add_argument(\"--input_model_path\", type=str, help=\"Path to the model\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Calling the main function\n",
    "    main(\n",
    "        input_data_path=args.input_data_path,\n",
    "        input_model_path=args.input_model_path\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the components into a pipeline\n",
    "\n",
    "Now that we have defined the components, we can combine them into a pipeline.\n",
    "\n",
    "We can chain the components together using command line arguments: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in all data: 300\n",
      "Number of rows in train data: 240\n",
      "Number of rows in test data: 60\n"
     ]
    }
   ],
   "source": [
    "!python components/train_test_split/component.py \\\n",
    "    --input_file_path data/data.csv \\\n",
    "    --train_test_ratio 0.8 \\\n",
    "    --train_output_path data/train.parquet \\\n",
    "    --test_output_path data/test.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in all data: 240\n"
     ]
    }
   ],
   "source": [
    "!python components/train/component.py \\\n",
    "    --input_data_path data/train.parquet \\\n",
    "    --output_model_path data/model.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in all data: 60\n",
      "MSE: 10.839162553892661\n"
     ]
    }
   ],
   "source": [
    "!python components/eval/component.py \\\n",
    "    --input_data_path data/test.parquet \\\n",
    "    --input_model_path data/model.pkl \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to do that is to wrap everything into Python code and the run the pipeline: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in all data: 300\n",
      "Number of rows in train data: 240\n",
      "Number of rows in test data: 60\n",
      "Number of rows in all data: 240\n",
      "Number of rows in all data: 60\n",
      "MSE: 10.18933520454265\n"
     ]
    }
   ],
   "source": [
    "# Importing the components \n",
    "from components.train_test_split.component import main as train_test_split\n",
    "from components.train.component import main as train\n",
    "from components.eval.component import main as eval\n",
    "\n",
    "# Defining the input data path \n",
    "input_data_path = 'data/data.csv'\n",
    "\n",
    "# Defining the train test split ratio\n",
    "train_test_ratio = 0.8\n",
    "\n",
    "# Defining the output paths\n",
    "train_output_path = 'data/train.parquet'\n",
    "test_output_path = 'data/test.parquet'\n",
    "\n",
    "# Defining the model output path\n",
    "model_output_path = 'data/model.pkl'\n",
    "\n",
    "# Running the components\n",
    "train_test_split(input_data_path, train_test_ratio, train_output_path, test_output_path)\n",
    "train(train_output_path, model_output_path)\n",
    "eval(test_output_path, model_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the components, the files that are created are: \n",
    "\n",
    "```bash\n",
    ".\n",
    "├── data \n",
    "│   ├── test.parquet\n",
    "│   ├── train.parquet\n",
    "│   └── model.pkl\n",
    "```\n",
    "\n",
    "And just like that, we have ran a small pipeline that encomposes components that only communicate through file paths. \n",
    "\n",
    "The same thing happens **exactly** the same way when we use Azure pipelines. \n",
    "\n",
    "## Test driven development \n",
    "\n",
    "Before going into the metadata definitions of the components and how to communicate them to azure, one thing to do before that is to localy test out components. We have done all the hard part right now, we only need to write some asserts and wrap everything into a function. \n",
    "\n",
    "A typical test might look like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in all data: 300\n",
      "Number of rows in train data: 240\n",
      "Number of rows in test data: 60\n",
      "Number of rows in all data: 240\n",
      "Number of rows in all data: 60\n",
      "MSE: 8.258892175893852\n",
      "Pipeline tests passed!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "def test_pipeline(): \n",
    "    # Arranging \n",
    "    input_data_path = 'data/data.csv'\n",
    "    train_test_ratio = 0.8\n",
    "    train_output_path = 'data/train.parquet'\n",
    "    test_output_path = 'data/test.parquet'\n",
    "\n",
    "    # Acting\n",
    "    train_test_split(input_data_path, train_test_ratio, train_output_path, test_output_path)\n",
    "    train(train_output_path, model_output_path)\n",
    "    eval(test_output_path, model_output_path)\n",
    "\n",
    "    # Asserting\n",
    "    assert os.path.exists(train_output_path), 'Train output path does not exist'\n",
    "    assert os.path.exists(test_output_path), 'Test output path does not exist'\n",
    "    assert os.path.exists(model_output_path), 'Model output path does not exist'\n",
    "    assert pd.read_parquet(train_output_path).shape[0] == 240, 'Train data shape is not correct'\n",
    "    assert pd.read_parquet(test_output_path).shape[0] == 60, 'Test data shape is not correct'\n",
    "\n",
    "    print('Pipeline tests passed!')\n",
    "\n",
    "# Running the test\n",
    "test_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "To sum up, we have learned:\n",
    "\n",
    "1. What are components\n",
    "2. How to define components\n",
    "3. How to chain components together\n",
    "4. How to test components\n",
    "\n",
    "In the next section we will learn how to define the metadata of the components and how to communicate them to Azure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}